\documentclass[11pt]{article}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage{color}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

\newcommand{\forceindent}{\leavevmode{\parindent=2em\indent}}

\newcounter{response}
%\newenvironment{response}[1][]{\color{blue}\refstepcounter{response}\noindent{{{\bf{Response \arabic{response}.}}}} \normalfont #1}
\newenvironment{response}[1][]{\color{blue}\refstepcounter{response}\noindent \normalfont #1}

\renewcommand{\theresponse}{{\bf Response \arabic{response}}}

\usepackage{url}

\title{Response to reviewer comments to the manuscript ``Software for the analysis and visualization of deep mutational scanning data'' submitted to \textit{BMC Bioinformatics}}
\author{Jesse D. Bloom}

\begin{document}

\maketitle

\begin{response}
Thanks to both reviewers for their careful review of the manuscript. I appreciate their attention to the manuscript and the software that it describes, and am glad that both reviewers see this software as useful for researchers using the new technique of deep mutational scanning.

The reviewer comments mostly focused on issues of clarification and questions about why and when certain approaches might be preferred. I have been able to address all of these comments in the revised manuscript, and I believe that the helpful comments and the changes made in response have substantially improved the manuscript.

Below I provide a point-by-point response to the comments.
\end{response}

\subsection*{Reviewer \#1}

Bloom describes a software package for analysis of deep mutational scanning using a novel (for this application) Bayesian/MCMC framework. The general framework appears to be sound and addresses a significant need for these new and highly promising functional assays. The manuscript is well-written and explains the major uses cases of the software in sufficient detail.

Minor comments/suggestions:

1. The notion that differences based on larger absolute counts provide more certainty than those based on small absolute counts is obviously true, but only up to a point. Since deep mutational scanning experiments typically involves exponential amplification of a finite number of selected molecules prior to sequencing, there will be a saturation point (or region) beyond which higher absolute counts no longer provide additional information. This effect might not be negligible in experiments that involve typical cultures of $10^6$ to $10^8$ independently transformed or transduced cells. I would encourage the author to note these effects and perhaps add features or provide guidance to software users for how to estimate or detect the saturation point.

2. On pg 2 – the notion of a mutation being ``beneficial'' is generally taken to be relative to the fitness of the wild-type (non-mutated) gene. Eq (1) is such that $\phi_{r,x} > 1$ does not imply that $x$ is beneficial in this sense. Consider the trivial example of a two site gene with two characters. The input library has one member with a neutral mutation at site 1, one member with a deleterious mutation at site 2 and one WT member. Post-selection $\phi$ for the non-WT character at site 1 will be $\frac{1/1}{1/2} > 1$ even though the non-WT character at site 1 is not beneficial. This should be clarified.

3. Given the substantial increase in runtime and computational complexity of the Bayesian/MCMC approach relative to the simple ratio approach (hours vs fraction of a second), it might be helpful to provide a bit more intuition about what is gained. Is it simply a ``regularization'' where high ratios obtained from low counts are pushed back towards the prior? If so, does the framework really provide more than what could be obtained with a much simpler regularization heuristic?

4. It’s not clear whether \textsl{dms\_inferprefs} outputs the probability of $\phi_{r,x} < 1$ (or $\phi_{r,x} \ne 0$). This is likely to be a useful feature.

5. It’s not clear whether the probability values for $\Delta\pi_{r,x} > 0$ or $< 0$ for \textsl{dms\_inferdiffprefs} should be interpreted as nominal or corrected. Providing false discovery rate estimates might be helpful for interpretation (also applies to 4.).

6. The function of \textsl{dms\_merge} isn’t explained in the paper. Is it simply averaging the two replicate profiles? Could additional statistical power be gained by explicitly modeling replicates within the Bayesian framework?

Level of interest: An article of importance in its field

Quality of written English: Acceptable

Statistical review: Yes, and I have assessed the statistics in my report.

\subsection*{Reviewer \#2}

The author describes a Python package, \textsl{dms\_tools}, which is used to infer character preferences using deep mutational scanning data. This software package allows the user to utilize the method described in Bloom 2014 (\textit{Mol Biol Evol}).

I found the paper easy to read and understand, and the algorithm is explained in a clear and logical manner. Simulations show clearly that this method is an improvement over using enrichment ratios to compute site-specific preferences. I have several questions that I would like the authors to comment on, all of which can be considered discretionary revisions:

Question 1:
On page 2, in the second paragraph, a deep mutational scanning study is described. The average mutation rate is $\mu=1/L$, where $L$ is the length of the gene. It is unclear to me why this is so -- intuitively one might think that a longer gene may have more mutations, given that mutation is a random process. More generally, could the author provide more details on the range of data one would expect from typical deep mutational scanning experiments?

\begin{response}
In principle, a deep mutational scanning experiment could introduce any number of mutations per gene. But typically, experimentalists tend to introduce an average of $\approx$1 mutation per gene when they create the codon-mutant libraries. The reason is that having genes with $\gg$1 mutations makes it hard to attribute the selection on any given gene variant to any specific mutation. For this reason, published deep mutational scanning experiments have generally targeted $\overline{\mu} \sim 1/L$. This is an important distinction between deep mutational scanning and naturally occurring mutations -- naturally occurring mutations will be more common in longer genes, since typically the per-site mutation rate is approximately uniform and longer genes have more sites. The revised text now clarifies this point.
\end{response}

Question 2:
The author states that for a depth of $10^6$, we expect about 2000 counts of non-wild type codons for the average gene. The author arrived at this number by multiplying the total number of reads by the probability of a mutation. This is reasonable if each read comes from a different DNA fragment, each with probability $\mu$ of mutating. However, if multiple reads all cover the same DNA fragment, would this relationship still hold? The author should comment on whether this situation is likely to occur in practice and, if so, how it affects the method.

Question 3:
Related to question 1: How does the author think that lower-quality data, either by experimental design limitations or by the variance in counts between sites, affect the inference and downstream interoperation of site-specific preferences? How sensitive is the algorithm to the prior for different qualities/quantities of data?

Question 4:
How robust is the method to the prior assumption that mutagenesis introduces all mutations at equal frequency? The author briefly discusses this issue on page 6, and I think it would be informative to address this question with an additional simulation. Does the present method still outperform the naive ratio method even when data are not generated under the model used in the inference?

\begin{response}
The reviewer notes that in a real experiment, the per-site per-character mutation ($\mu_{r,x}$) and error rates ($\epsilon_{r,x}$ and $\rho_{r,x}$) will not be completely uniform. However, the prior assumption used by \textsl{dms\_tools} is that the these rates are uniform (although this prior is expressed fairly loosely by a Dirichlet with a relatively modest concentration parameter). The reviewer is therefore rightly pointing out that if the simulations are performed with uniform mutation (and error) rates, then the inference on the simulated data might work better than it would on real data.

Although the original manuscript text was unclear on this point, in fact the simulations were performed with non-uniform mutation and error rates. This was done by drawing the rates from Dirichlet distributions parameterized such that there was substantial variation across sites and characters. The unevenness in the simulated rates is controlled by the concentration parameter of the Dirichlet, and the simulations were performed with a concentration parameter that gave an unevenness roughly comparable to that observed in real codon-mutagenesis experiments. The code implementing this aspect of the simulations is at \url{????}. 

The unevenness in the simulated mutation rates can be grasped in intuitive terms by examining the simulated counts in \url{simulated_errs_depth_2e+06_pre.txt}. This simulation is of a library sequenced at depth $2 \times 10^6$ and has an average per-site codon mutation rate of $\overline{\mu} = \frac{1}{264}$, meaning that the average mutation rate to any given one of the 63 mutant codons at any given site is $\frac{1}{264 \times 63} \approx 6 \times 10^{-5}$. Therefore, with perfectly uniform mutation rates and no sampling error, each mutant codon would be observed $\frac{2 \times 10^6}{264 \times 63} \approx 120$ times. In the simulated data in \url{simulated_errs_depth_2e+06_pre.txt}, the counts for specific mutations range from 0 counts to 1400 counts, with a 10th percentile of 18 counts, a 25th percentile of 43 counts, a 75th percentile of 174 counts, and a 90th percentile of 275 counts. Therefore, the unevenness in the simulated data reasonably reflects that of a real experiment, and so these data already represents a fair test for the inferences.

This issue has been clarified in the text by clearly stating that the simulations were done at uneven mutation and error rates.
\end{response}

Level of interest: An article whose findings are important to those with closely related research interests

Quality of written English: Acceptable

Statistical review: No, the manuscript does not need to be seen by a statistician.


\end{document}  